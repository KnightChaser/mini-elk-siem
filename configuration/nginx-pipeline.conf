# /etc/logstash/conf.d/nginx-pipeline.conf

input {
    beats {
        port => 5044
    }
}

filter {
    # Process only nginx access logs
    if [log_type] == "nginx_access" {
        # Parse nginx access log entries
        grok {
            match => {
                "message" => "%{IPORHOST:client_ip} - %{DATA:identity} %{DATA:user} \[%{HTTPDATE:log_timestamp}\] \"%{WORD:http_method} %{URIPATHPARAM:request_path} HTTP/%{NUMBER:http_version}\" %{NUMBER:status_code} (?:%{NUMBER:response_size}|-)"
            }
            # Tag entries that fail parsing for further investigation
            tag_on_failure => ["_nginx_access_grok_failure"]
        }

        # Extract query parameters from the request path (if present)
        kv {
            source => "[request_path]"
            field_split => "&?"
            value_split => "="
            prefix => "query_param_"
        }

        # Convert timestamp into @timestamp for time-based analysis
        date {
            match => [ "log_timestamp", "dd/MMM/yyyy:HH:mm:ss Z" ]
            target => "@timestamp"
            remove_field => [ "log_timestamp" ]
        }

        # Add some context for debugging or additional enrichment
        mutate {
            add_field => {
                "pipeline" => "nginx_access_logs"
                "source_type" => "nginx"
            }
            # Rename fields for better readability
            rename => { 
                "status_code" => "http_status" 
                "response_size" => "response_bytes"
            }
        }

        # Handle parsing errors for debugging
        if "_nginx_access_grok_failure" in [tags] {
            mutate {
                add_field => {
                    "error_reason" => "Failed to parse nginx access log"
                }
            }
        }
    }
}

output {
    # Forward the processed logs to the Python client
    tcp {
        host => "127.0.0.1"
        port => 9999
    }
}

